{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract A2D2 Metadeta using EMR\n",
    "\n",
    "This example extracts [A2D2](https://registry.opendata.aws/aev-a2d2/) metadata about various drives. The extracted drive metadata is saved in CSV files in an S3 bucket. The CSV files can be used to copy the metatadata into Amazon Redshift database.\n",
    "\n",
    "This example requires that this Jupyter notebook is running in a Jupyter notebook instance attached to a Spark cluster. In AWS, we have two options for attaching a Jupyter notebook instance to a Spark cluster.\n",
    "\n",
    "#### Option 1: Create Notebook instance attached to AWS Glue Development Endpoint\n",
    "\n",
    "The first option is to create an Amazon SageMaker Notebook instance attached to an [AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint.html), as described below:\n",
    "\n",
    "  -  [Add an AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/add-dev-endpoint.html)\n",
    "      - We recommend 5 G.2X workers for a total of 11 data processing units\n",
    "  -  [Use an Amazon SageMaker Notebook with your AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-sage.html)\n",
    "  \n",
    "\n",
    "#### Option 2: Create Notebook instance attached to Amazon EMR\n",
    "\n",
    "The second option is to create an [Amazon EMR Cluster](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-launch.html) using <b>Advanced Options</b> to include <b>Spark and Livy</b> in the software for the EMR cluster, and [using Amazon EMR notebooks](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks.html) to run this exmaple.\n",
    "\n",
    "To get started quickly, we recommend Option 1 noted above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session\n",
    "The first step is to import required ```pyspark``` packages. This step implicitly initializes a Spark session, which is available in a variable named ```spark```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define JSON schema\n",
    "The next step is to define the schema for readding the metadata from JSON files. Even though Spark can implicitly detect the schema rom JSON files, it is a very slow process when the number of files is large, as is the case for the A2D2 datatset. Because we do not need all the fields in the JSON file, we define the schema for relevant data. This enables the JSON files to be read relatively quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "# Create a schema for the dataframe\n",
    "schema = StructType([\n",
    "    StructField('cam_name', StringType(), True),\n",
    "    StructField('cam_tstamp', LongType(), True),\n",
    "    StructField('image_png', StringType(), True),\n",
    "    StructField('pcld_npz', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON files\n",
    "Next, we read the reelvant A2D2 dataset JSON files into a Spark ```DataFrame```. The files we are interested in are under the ```.../camera_lidar/``` prefix path. Under this path, there is data from various drives done on different days. For each of these drives, there is image and point-cloud data. We read the relevant metadata from JSON files below. Notice the wildcards in the S3 prefix below.\n",
    "\n",
    "Set```s3_bucket``` below to your S3 bucket name. The cell below may take approximately 5 - 10 minutes to run: the exact time depends on the vertical and horizontal scale of your Glue development endpoint, or your EMR cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket= # set your bucket name below\n",
    "s3_prefix = \"a2d2/camera_lidar/*/camera/*/*.json\" \n",
    "\n",
    "s3_uri = f\"s3a://{s3_bucket}/{s3_prefix}\"\n",
    "df=spark.read.json(s3_uri, schema, multiLine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check below to see we read the JSON file data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we clean the data, dropping rows with missing values in any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with any null value\n",
    "df_clean=df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the cleaned up data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.printSchema()\n",
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Spark user defined fucntions (UDFs) \n",
    "Below we define relevant Spark UDFs that we need to extract relevant information and save it into CSV files. \n",
    "\n",
    "#### Scene Id UDF\n",
    "First we define a UDF for extracting ```scene_id```. Each image and point cloud file starts with a prefix that identifies the date and time of the test-drive, and we use this information as the ```scene_id```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def scene_id(file_name):\n",
    "    parts=file_name.split('_')\n",
    "    return parts[0]\n",
    "\n",
    "scene_id_udf = udf(scene_id, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 Key UDF\n",
    "Next, we define a UDF to build the Amazon S3 object path for the image or point cloud file stored in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_key(file_name, cam_name):\n",
    "    parts=file_name.split('_')\n",
    "    s3_key = f\"a2d2/camera_lidar/{parts[0][0:8]}_{parts[0][8:]}/{parts[1]}/cam_{cam_name}/{file_name}\"\n",
    "    return s3_key\n",
    "\n",
    "s3_key_udf = udf(s3_key, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensor Id UDF\n",
    "Next, we define a UDF for the sensor id. The convention we use for the sensor id is that it starts with a prefix of ```camera/``` for image sensors and ```lidar/``` for point cloud sensors. This prefix is followed bu the location of the sensors, such as, ```front_left```, ```side_left```, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_id(file_name, cam_name):\n",
    "    parts=file_name.split('_')\n",
    "    sensor_id = f\"{parts[1]}/{cam_name}\"\n",
    "    return sensor_id\n",
    "\n",
    "sensor_id_udf = udf(sensor_id, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vehicle Id UDF\n",
    "Next, we define a UDF for vehicle id. This is a trivial UDF as it always returns a hard-coded value of ```a2d2``` as the vehicle id for all test-drive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle id is hard-coded as `a2d2` in this example\n",
    "def vehicle_id():\n",
    "    return \"a2d2\"\n",
    "\n",
    "vehicle_id_udf = udf(vehicle_id, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3 Bucket Name UDF\n",
    "Next, we define a UDF that provides the bucket name. This is a trivial UDF as it always returns a hard-coded bucket name. **You should set the hard-coded bucket name below to your bucket name.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_bucket_name():\n",
    "    bucket_name = # set it to your bucker name\n",
    "    return bucket_name\n",
    "s3_bucket_name_udf = udf(s3_bucket_name, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrames for Image and Point-cloud files\n",
    "Below we create the Spark DataFrame containing metadata for the image files. Notice the use of UDFs we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image = df_clean.select(vehicle_id_udf().alias(\"vehicle_id\"), \n",
    "                      scene_id_udf(df_clean.image_png).alias('scene_id'), \n",
    "                      sensor_id_udf(df_clean.image_png, df_clean.cam_name).alias('sensor_id'),\n",
    "                      df.cam_tstamp.alias('data_ts'),\n",
    "                      s3_bucket_name_udf().alias('s3_bucket'),\n",
    "                      s3_key_udf(df_clean.image_png, df_clean.cam_name).alias('s3_key'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate the image file DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image.printSchema()\n",
    "df_image.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save the image DataFrame as CSV files to Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prepared data frame S3 bucket\n",
    "df_image.write.save(f\"s3://{s3_bucket}/emr/a2d2/image/v1\", format='csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the Spark DataFrame for point-cloud files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pcld = df_clean.select(vehicle_id_udf().alias(\"vehicle_id\"), \n",
    "                      scene_id_udf(df_clean.pcld_npz).alias('scene_id'), \n",
    "                      sensor_id_udf(df_clean.pcld_npz, df_clean.cam_name).alias('sensor_id'),\n",
    "                      df.cam_tstamp.alias('data_ts'),\n",
    "                      s3_bucket_name_udf().alias('s3_bucket'),\n",
    "                      s3_key_udf(df_clean.pcld_npz, df_clean.cam_name).alias('s3_key'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we validate the DataFrame for point-cloud files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pcld.printSchema()\n",
    "df_pcld.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save the point-cloud DataFrame as CSV files into the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pcld.write.save(f\"s3://{s3_bucket}/emr/a2d2/pcld/v1\", format='csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You can use the CSV files saved in S3 bucket to load this metadata into Amazon Redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
